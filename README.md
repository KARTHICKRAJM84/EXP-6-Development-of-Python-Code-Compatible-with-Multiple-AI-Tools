# EXP-6-Development-of-Python-Code-Compatible-with-Multiple-AI-Tools
## Name:KARTHICK RAJ M
## Reg.no:212221040073

# Aim:
To compare the responses of two open-source language models, GPT-Neo and GPT-2, to a 
given question, and analyze how different models generate text and handle natural language 
queries.
# Procedure:
1. Install Required Libraries:
Use the command below to install the necessary Python libraries:
bash
Copy code
pip install transformers torch
2. Load Models:
○ Load two pre-trained language models from Hugging Face:
■ GPT-Neo (EleutherAI/gpt-neo-1.3B).
■ GPT-2 (gpt2).
3. Define Functions:
○ Define two functions to generate text from both models.
■ GPT-Neo Function: Generates text from the GPT-Neo model.
■ GPT-2 Function: Generates text from the GPT-2 model.
4. Generate Answers:
○ Input the question “What are the benefits of renewable energy?” to both models 
and generate their responses.
5. Compare Answers:
○ Compare the generated answers from both models to see if they match or differ.
○ Print the responses and a summary indicating whether the answers are the same 
or different.
6. Execute the Code:
○ Run the code to generate and compare answers.

# Deliverables:
1. Python Script: A script to compare answers from two models.
2. Comparison Output: The answers generated by both models and a summary of 
whether the answers are similar or different.
Sample Code:
from transformers import pipeline
# Load GPT-Neo and GPT-2 models
generator_neo = pipeline('text-generation', model='EleutherAI/gpt-neo-
1.3B')
generator_gpt2 = pipeline('text-generation', model='gpt2')
# Function to get answer from GPT-Neo
def get_gpt_neo_answer(question):
 generated_text = generator_neo(question, max_length=100, 
num_return_sequences=1)
 return generated_text[0]['generated_text']
# Function to get answer from GPT-2
def get_gpt2_answer(question):
 generated_text = generator_gpt2(question, max_length=100, 
num_return_sequences=1)
 return generated_text[0]['generated_text']
# Function to compare answers from both models
```
def compare_answers(question):
 answer_gpt_neo = get_gpt_neo_answer(question)
 answer_gpt2 = get_gpt2_answer(question)
 
 print("GPT-Neo Answer:", answer_gpt_neo)
 print("GPT-2 Answer:", answer_gpt2)
 
 if answer_gpt_neo == answer_gpt2:
 summary = "Both models provided the same answer."
 else:
 summary = "The answers are different."
 
 print("Summary:", summary)

 
 return {
 "question": question,
 "gpt_neo_answer": answer_gpt_neo,
 "gpt2_answer": answer_gpt2,
 "summary": summary
 }
Run the comparison with a sample question
question = " write about renewable energy "
result = compare_answers(question)
print("Comparison Result:", result)
```
Result:
Sample Output:

# Experiment Summary: Comparison of GPT-Neo and GPT-2
The experiment involved comparing the performance of GPT-Neo and GPT-2 using the same 
input prompts to evaluate their outputs. The key findings were:
1. Output Precision:
o GPT-2 provided more precise and contextually relevant responses.
o GPT-Neo's results were comparatively less focused and occasionally deviated 
from the context.
2. Qualitative Observations:
o GPT-2 excelled in maintaining coherence and relevance across longer 
responses.
o GPT-Neo showed more variability, which sometimes led to creativity but at the 
cost of precision.

The results highlight the differences in design philosophies and training datasets between the 
models, with GPT-2 favoring tighter control over response generation. This suggests GPT-2 
might be more suited for applications requiring accuracy and relativity, whereas GPT-Neo might 
be better for tasks demanding flexibility or creative exploration.
# Conclusion:
In this experiment, we compared the responses of two different language models, GPT-Neo and 
GPT-2, to the same input question. The results showed that the models provided different 
answers, which indicates that each model has its unique way of generating text based on its 
training data and architecture.
